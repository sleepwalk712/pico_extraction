{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1ba897-c51f-4ade-8732-8bfed3e11412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /models/Meta-Llama-3-8B-Instruct.Q5_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.21 GiB (5.57 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   344.44 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4987.99 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LAMMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session already created, some configs may not take.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "sent_small_bert_L2_128 download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/28 11:10:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/04/28 11:10:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 16.1 MB\n",
      "[OK!]\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      " The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or the mechanisms by which their involvement may have influenced breastfeeding outcomes. Therefore, it is unclear what specific aspects of father inclusion are most critical for improving breastfeeding outcomes.  # Answer: The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or the mechanisms by which their involvement may have influenced breastfeeding outcomes. Therefore, it is unclear what specific aspects of father inclusion are most critical for improving breastfeeding outcomes.  # Answer: The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     426.89 ms\n",
      "llama_print_timings:      sample time =     135.25 ms /   256 runs   (    0.53 ms per token,  1892.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     426.59 ms /   472 tokens (    0.90 ms per token,  1106.45 tokens per second)\n",
      "llama_print_timings:        eval time =    3067.00 ms /   255 runs   (   12.03 ms per token,    83.14 tokens per second)\n",
      "llama_print_timings:       total time =    4938.41 ms /   727 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or the mechanisms by which their involvement may have influenced breastfeeding outcomes. Therefore, it is unclear what specific aspects of father inclusion are most critical for improving breastfeeding outcomes.  # Answer: The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or the mechanisms by which their involvement may have influenced breastfeeding outcomes. Therefore, it is unclear what specific aspects of father inclusion are most critical for improving breastfeeding outcomes.  # Answer: The study found that both groups had co-parents involved and invited both partners to work together. Therefore, the study's findings tend to support the literature suggesting that including fathers in breastfeeding interventions can increase breastfeeding outcomes. The study does not provide information on the specific ways in which fathers were included in the breastfeeding interventions or\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.johnsnowlabs import JohnSnowLabsEmbeddings\n",
    "\n",
    "question = \"What is the intervention between groups in this study?\"\n",
    "pdf = \"Abbass-Dick-2020-32739716.pdf\"\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1\n",
    "n_batch = 512\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/models/Meta-Llama-3-8B-Instruct.Q5_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx=4096,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\"/data/\" + pdf)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits, embedding=JohnSnowLabsEmbeddings())\n",
    "\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages\n",
    "\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34be674-3e21-4a7d-a9b0-fb54f21fa934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='provided  a lot of valuable  information,  it was helpful  to have re- \\nminders  to keep me on track as it reminded  me the resource  ex- \\nisted.”\\nDiscussion  \\nThe purpose  of this randomized  controlled  trial was to com- \\npare the effectiveness  of two study conditions  (Study Condition  \\n#1: eHealth  Resource  as a Supplemental  Resource  or Study Con-', metadata={'page': 5, 'source': '/data/Abbass-Dick-2020-32739716.pdf'}),\n",
       " Document(page_content='J. Abbass-Dick,  W. Sun and A. Newport et al. / Midwifery  90 (2020) 102812 3 \\nto the couple that described  the eHealth  resource,  its contents  \\nand how to access it and this was reviewed  in detail as the cou- \\nple followed  along while viewing  the resource.  The resource  con- \\ntained comprehensive  breastfeeding  information  which was orga-', metadata={'page': 2, 'source': '/data/Abbass-Dick-2020-32739716.pdf'}),\n",
       " Document(page_content='eHealth  breastfeeding  co-parenting  website,  which they could ac- \\ncess independently  throughout  the perinatal  period.  To receive  \\nthis access, a telephone  or web-based  meeting  was held with \\na research  assistant  where the resource  was reviewed  with the', metadata={'page': 1, 'source': '/data/Abbass-Dick-2020-32739716.pdf'}),\n",
       " Document(page_content='and use of the eHealth  resource  which was assessed  at all follow \\nup time points. \\nBreastfeeding  Resource  Use. Information  regarding  breastfeeding  \\nresources  that participants  utilized  in the community  were col- \\nlected by each group by asking them to select which they had used \\nfrom a list provided  (books,  videos,  websites  etc.) and how helpful', metadata={'page': 3, 'source': '/data/Abbass-Dick-2020-32739716.pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5181fd-3a9a-43e0-a99c-8779084dc35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
